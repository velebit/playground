#!/usr/bin/python3

import argparse
import bs4
import datetime as dt
import dateutil.parser as dp
# import feedgen.util as fu
import glob
import locale
import os
import re
import requests
import sys
# import time as tm


def format_rfc2822(date_time):
    old_locale = locale.setlocale(locale.LC_ALL)
    locale.setlocale(locale.LC_ALL, 'C')
    rfc2822_date = date_time.strftime('%a, %d %b %Y %H:%M:%S %z')
    locale.setlocale(locale.LC_ALL, old_locale)
    return rfc2822_date


def canonical_datetime(date):
    try:
        parsed = dp.parse(date + " 00:00 +0000")
    except Exception:
        parsed = dp.parse(date)
    return dt.datetime.fromtimestamp(parsed.timestamp(), tz=dt.timezone.utc)


def canonical_page_url(url):
    return re.sub(r'/+$', '', url) + '/'


def get_audio_book_title(node):
    return node.find(class_='main-content').h1.get_text(strip=True)


def get_link_url(node, link_text):
    return node.find('a', href=True, string=link_text)['href']


def get_catalog_date(node):
    date_node = node.find(string='Catalog date:').parent
    while date_node.name != 'dd':
        date_node = date_node.next_sibling
    return date_node.text


def get_rss_channel_url(rss_source):
    rss_xml = bs4.BeautifulSoup(rss_source, features="xml")
    return rss_xml.channel.link.get_text(strip=True)


def check_page_url_matches(specified_url, rss_url):
    if canonical_page_url(specified_url) == canonical_page_url(rss_url):
        return True
    print(f"Warning: page {specified_url} refers to RSS linked to {rss_url}",
          file=sys.stderr)
    return False


page_url_to_file = None


def set_existing_file(page_url, filename):
    assert page_url_to_file is not None
    page_url_to_file[page_url] = filename


def get_existing_file(page_url):
    assert page_url_to_file is not None
    if page_url not in page_url_to_file:
        return None
    return page_url_to_file[page_url]


def scan_urls_in_existing_rss():
    global page_url_to_file
    page_url_to_file = {}
    print("Scanning RSS files for existing source URLs"
          " (--no-scan-existing to skip)", file=sys.stderr)
    for rss_file in glob.glob("../docs/feeds/librivox-*.rss"):
        with open(rss_file, 'rb') as f:
            set_existing_file(get_rss_channel_url(f), rss_file)
    print(f"Scanned {len(page_url_to_file)} RSS files", file=sys.stderr)


def process_url(page_url, *, settings):
    page_response = requests.get(page_url)
    page_response.raise_for_status()
    page_html = bs4.BeautifulSoup(page_response.content, features="lxml")
    title = get_audio_book_title(page_html)
    if settings.name is not None:
        name = settings.name
    else:
        name = re.sub(r'(?<=\w)[:;].*$', '', title.lower())
        name = re.sub(r"'s\b", "", name)
        name = re.sub(r'\W+$', '', re.sub(r'^\W+', '', name))
        name = re.sub(r'\W+', '-', name)
    rss_url = get_link_url(page_html, 'RSS')
    lvid = re.sub(r'\D+', '', re.sub(r'^.*/', '', rss_url))
    image_url = get_link_url(page_html, 'Download cover art')
    cat_date = canonical_datetime(get_catalog_date(page_html))
    if settings.id_last:
        file_base = f"librivox-{name}-{lvid}"
    else:
        file_base = f"librivox-{lvid}-{name}"
    file_orig = f"{file_base}-orig.rss"
    file_done = f"../docs/feeds/{file_base}.rss"

    rss_response = requests.get(rss_url)
    rss_response.raise_for_status()
    rss_content = rss_response.content
    rss_page_url = get_rss_channel_url(rss_content)
    check_page_url_matches(page_url, rss_page_url)
    # save original before checking for uniqueness
    if settings.save_orig:
        with open(file_orig, 'wb') as f:
            f.write(rss_content)
        print(f"Wrote {file_orig}", file=sys.stderr)
    else:
        try:
            os.remove(file_orig)
        except FileNotFoundError:
            pass
    if settings.scan_existing:
        existing = get_existing_file(rss_page_url)
        if existing is not None:
            # TODO Or use the old file name? Maybe as an option?
            print(f"Skipping {page_url}, already in {existing}",
                  file=sys.stderr)
            return

    next_date = cat_date

    def make_next_date_bytes(match):
        nonlocal next_date
        fdate = format_rfc2822(next_date)
        next_date += dt.timedelta(minutes=+1)
        return b'<pubDate>' + bytes(fdate, 'utf-8') + b'</pubDate>'
    rss_content = re.sub(rb'(?:<!--)?<pubDate>[^<>]*</pubDate>(?:-->)?',
                         make_next_date_bytes, rss_content)

    if m := re.search(rb'^(\s*)<!-- file loop -->', rss_content, flags=re.M):
        prefix = m.group(1)
        frag_xml = bs4.BeautifulSoup(features="xml")
        tag_image = frag_xml.new_tag("image")
        tag_image.append(frag_xml.new_tag("url"))
        tag_image.url.string = image_url
        tag_iimage = frag_xml.new_tag("itunes:image", href=image_url)
        tag_iexpl = frag_xml.new_tag("itunes:explicit")
        tag_iexpl.string = "no"
        rss_content = re.sub(rb'^(?=\s*<!-- file loop -->)',
                             b"".join([prefix + x.encode('utf-8') + b'\n'
                                       for x in (tag_image, tag_iimage,
                                                 tag_iexpl)]),
                             rss_content, flags=re.M)
    else:
        rss_xml = bs4.BeautifulSoup(rss_content, features="xml")
        tag_image = rss_xml.new_tag("image")
        tag_image.append(rss_xml.new_tag("url"))
        tag_image.url.string = image_url
        tag_iimage = rss_xml.new_tag("itunes:image", href=image_url)
        tag_iexpl = rss_xml.new_tag("itunes:explicit")
        tag_iexpl.string = "no"
        rss_xml.item.insert_before(tag_image, tag_iimage, tag_iexpl)
        rss_content = rss_xml.encode('utf-8')
    with open(file_done, 'wb') as f:
        f.write(rss_content)
    set_existing_file(rss_page_url, file_done)
    print(f"Wrote {file_done}", file=sys.stderr)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('-v', '--verbose', action='count', default=0)
    parser.add_argument('-n', '--name', action='store')
    parser.add_argument('-i', '--id-last', action='store_true')
    parser.add_argument('-o', '--save-orig', action='store_true')
    parser.add_argument('--scan-existing', default=True,
                        action=argparse.BooleanOptionalAction)
    parser.add_argument('urls', metavar='LIBRIVOX_URL', nargs='+')
    settings = parser.parse_args()
    return settings


def main():
    settings = parse_args()
    if settings.scan_existing:
        scan_urls_in_existing_rss()
    for url in settings.urls:
        process_url(url, settings=settings)


if __name__ == "__main__":
    main()
