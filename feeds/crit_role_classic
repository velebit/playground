#!/usr/bin/python3

import copy as cp
import datetime as dt
import feedparser as fi
import feedgen.feed as fo
import pickle
import regex as re
import urllib.parse

FEED_DATA_CACHE_FILE = 'cache.pkl'

FEED_URLS = (
    'https://feed.podbean.com/geekandsundry/feed.xml',
    'https://anchor.fm/s/91c7948/podcast/rss',
)

OUTPUT_URI_BASE = 'https://velebit.github.io/'
OUTPUT_DIR = 'feeds/'
OUTPUT_TOP_PATH = '../docs/'

def try_read_cache(cache_file):
    try:
        with open(cache_file, 'rb') as cache:
            feed_data = pickle.load(cache)
            print(f"Pickled data cache read from '{cache_file}'")
            return feed_data
    except:
        return None

def try_write_cache(cache_file, feed_data):
    try:
        with open(cache_file, 'wb') as cache:
            pickle.dump(feed_data, cache)
            print(f"Pickled data cache written to '{cache_file}'")
    except:
        print(f"Warning: could not write feed data to '{cache_file}'")

def get_and_parse_feeds(feed_urls):
    feed_data = [fi.parse(f) for f in feed_urls]
    print(f"Feed data downloaded and parsed")
    return feed_data

def read_feeds(*, feed_urls, cache_file=FEED_DATA_CACHE_FILE):
    assert type(feed_urls) is not str, \
        "feed_urls should be a container of strings"
    feed_data = None
    if cache_file is not None:
        feed_data = try_read_cache(cache_file)
    if feed_data is None:
        feed_data = get_and_parse_feeds(feed_urls)
        if feed_data is not None and cache_file is not None:
            try_write_cache(cache_file, feed_data)
    return feed_data

def _dump_skeleton(fd):
    import pprint as pp
    fd = cp.deepcopy(fd)
    for d in fd:
        if len(d['entries']) > 1:
            d['entries'][:] = d['entries'][0:1]
    pp.pp(fd, indent=4)

def feed_rss_filename(*, campaign):
    return f"{OUTPUT_DIR}CritRole_campaign_{campaign}.rss"

def logo_filename(*, campaign):
    return f"{OUTPUT_DIR}CritRole_campaign_{campaign}.jpg"

def generate_monotonic_datetime(entries):
    # convert parsed datetime as a date
    for e in entries:
        assert e['published_parsed'].tm_gmtoff is None
        e['published_datetime'] = \
            dt.datetime(*(e['published_parsed'][0:6]),
                        tzinfo=dt.timezone.utc)
        e['monotonic_datetime'] = e['published_datetime']
    # make monotonic_datetime always increase
    for i in range(1, len(entries)):
        if (entries[i]['monotonic_datetime']
            < entries[i-1]['monotonic_datetime']):
            entries[i]['monotonic_datetime'] = \
                entries[i-1]['monotonic_datetime'] + \
                dt.timedelta(minutes=+1)
    return entries

def extract_entries(feed_urls=FEED_URLS, *, cache_file=FEED_DATA_CACHE_FILE):
    feed_data = read_feeds(feed_urls=feed_urls, cache_file=cache_file)
    if False:
        _dump_skeleton(feed_data)
    cr1_entries = []
    cr2_entries = []
    for feed in feed_data:
        for entry in feed['entries']:
            title = entry['title']
            if re.search(r'Talks Machina', title):
                pass  # reject
            elif re.search(r'^Talking ', title):
                pass  # reject
            elif re.search(r'One-Shot', title):
                pass  # reject
            elif match := re.search(r'^Vox Machina Ep\. (\d+)', title):
                entry = cp.deepcopy(entry)
                entry['feed_sort_order'] = int(match.group(1))
                cr1_entries.append(entry)
            elif match := re.search(r'^Campaign Wrap-Up$', title):
                entry = cp.deepcopy(entry)
                entry['feed_sort_order'] = 9999
                cr1_entries.append(entry)
            elif match := re.search(r'^C2E(\d+)', title):
                entry = cp.deepcopy(entry)
                entry['feed_sort_order'] = int(match.group(1))
                cr2_entries.append(entry)
            elif match := re.search(r'Campaign 2 Wrap Up$', title):
                entry = cp.deepcopy(entry)
                entry['feed_sort_order'] = 9900
                cr2_entries.append(entry)
            elif match := re.search(r'The Mighty Nein Reunited Part (\d+)',
                                    title):
                entry = cp.deepcopy(entry)
                entry['feed_sort_order'] = 9910 + int(match.group(1))
                cr2_entries.append(entry)
            elif match := re.search(r'^C3E(\d+)', title):
                pass  # reject
            elif match := re.search(r'Exandria Unlimited', title):
                pass  # reject
            elif match := re.search(r'4-Sided Dive', title):
                pass  # reject
            elif re.search(r'(?:Critmas|Cast Q&A|Fireside Chat)', title):
                pass  # reject
            elif re.search(r'^(?:Honey Heist|UnDeadwood|Crash Pandas)', title):
                pass  # reject
            elif re.search(r'^(?:The Search For |A Familiar Problem)', title):
                pass  # reject
            elif re.search(r'(?:Call of Cthulhu|Dalen\'s Closet)', title):
                pass  # reject
            elif re.search(r'(?:Vox Machina vs\. |Guest Battle)', title):
                pass  # reject
            elif re.search(r'(?:Darrington Brigade)', title):
                pass  # reject
            elif re.search(r'^(?:Game Masters|Dignity)', title):
                pass  # reject
            else:
                print(f"Unknown title: {title}")
    for entries in (cr1_entries, cr2_entries):
        entries.sort(key=lambda e: e['feed_sort_order'])
        generate_monotonic_datetime(entries)
    return (cr1_entries, cr2_entries)

def make_crit_role_campaign_feed(*, campaign, author=None, pub_time=None,
                                 feed_urls=None):
    feed = fo.FeedGenerator()
    feed_uri = OUTPUT_URI_BASE + feed_rss_filename(campaign=campaign)
    logo_uri = OUTPUT_URI_BASE + logo_filename(campaign=campaign)
    feed.id(feed_uri)
    feed.load_extension('podcast')  # NOTE: only works with RSS, not Atom!
    feed.link(href=feed_uri, rel='self')
    #feed.link(href=???, rel='alternate')
    title = f"Critical Role campaign {campaign}"
    feed.title(title)
    subtitle = ('A bunch of nerdy-ass voice actors[tm] playing TTRPGs.  ' +
                f"This feed collects all {title} episodes")
    if feed_urls is not None:
        sources = " and/or ".join(feed_urls)
        subtitle += f" originally published at {sources}"
    subtitle += '.'
    feed.subtitle(subtitle)
    feed.podcast.itunes_image(logo_uri)
    feed.logo(logo_uri)
    feed.image(logo_uri)
    if author is not None:
        feed.author(author)
    else:
        feed.author({'name': 'Critical Role', 'email': 'music@critrole.com'})
    feed.language('en')
    feed.podcast.itunes_category([{'cat': 'Leisure', 'sub': 'Games'},
                                  {'cat': 'Leisure'}, {'cat': 'Games'}])
    feed.category([{'term': t, 'scheme': 'http://www.itunes.com/'}
                   for t in ('Leisure:Games', 'Leisure', 'Games',
                             'critical role', 'dnd', 'role playing game',
                             'ttrpg', 'dungeons and dragons', 'd and d',
                             'actual play', 'rpg')])
    feed.copyright('Copyright 2015-2021 Geek and Sundry and/or Critical Role.')
    feed.podcast.itunes_block(False)
    feed.podcast.itunes_explicit('no')
    if pub_time is not None:
        feed.pubDate(pub_time)
    return feed

def populate_entry_data(entry_data, image, feed_entry):
    feed_entry.title(entry_data['title'])
    for l in entry_data['links']:
        if l['rel'] != 'self':
            feed_entry.link(l)
    if 'comments' in entry_data:
        feed_entry.comments(entry_data['comments'])
    feed_entry.pubDate(entry_data['monotonic_datetime'])
    feed_entry.updated(entry_data['monotonic_datetime'])
    feed_entry.id(entry_data['id'])
    if 'content' in entry_data and 'value' in entry_data['content']:
        feed_entry.content(entry_data['content']['value'])
    if 'summary' in entry_data:
        feed_entry.summary(entry_data['summary'])
    elif 'subtitle' in entry_data:
        feed_entry.summary(entry_data['subtitle'])
    feed_entry.author(entry_data['authors'])
    feed_entry.podcast.itunes_image(image)
    if 'itunes_duration' in entry_data:
        feed_entry.podcast.itunes_duration(entry_data['itunes_duration'])
    #feed_entry.podcast.itunes_...(...)

def generate_feeds(entries_lists, *, feed_urls=None):
    now = dt.datetime.now(tz=dt.timezone.utc)
    cr1 = make_crit_role_campaign_feed(campaign="1", pub_time=now,
                                       author={'name': 'geekandsundry'},
                                       feed_urls=feed_urls)
    cr2 = make_crit_role_campaign_feed(campaign="2", pub_time=now,
                                       feed_urls=feed_urls)
    feeds = (cr1, cr2)
    for feed, entries in zip(feeds, entries_lists):
        for entry in entries:
            populate_entry_data(entry, feed.logo(),
                                feed.add_entry(order='prepend'))
    return feeds

def write_feeds(feeds, *, top_path=OUTPUT_TOP_PATH):
    for feed in feeds:
        subpath = re.sub(r'^/+', '', urllib.parse.urlparse(feed.id()).path)
        path = top_path + subpath
        feed.rss_file(path)
        print(f"RSS data written to '{path}'")

def main():
    feed_urls = FEED_URLS
    write_feeds(generate_feeds(
        extract_entries(feed_urls=feed_urls, cache_file=None),
        feed_urls=feed_urls))

if __name__ == "__main__":
    main()
